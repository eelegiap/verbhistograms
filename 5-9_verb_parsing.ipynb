{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the RNC to look at preps in certain windows of verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98892"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_morphoru_rnc\n",
    "\n",
    "path = 'RNCgoldInUD_Morpho.conll'\n",
    "records = load_morphoru_rnc(path)\n",
    "rnccorpus = []\n",
    "for record in records:\n",
    "    rnccorpus.append(record)\n",
    "len(rnccorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep + case!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_sentence(tokens, upperindexlist):\n",
    "#     sentence = ''\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         if token.pos == 'PUNCT':\n",
    "#             space = ''\n",
    "#         else:\n",
    "#             space = ' '\n",
    "#         if i in upperindexlist:\n",
    "#             text = token.text.upper()\n",
    "#         else:\n",
    "#             text = token.text\n",
    "#         if text == None:\n",
    "#             text = ''\n",
    "#         sentence = sentence + space + text\n",
    "#     return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorphoToken(\n",
       "    text='угодить',\n",
       "    lemma='угодить',\n",
       "    pos='VERB',\n",
       "    feats={'VerbForm': 'Inf', 'Voice': 'Act'},\n",
       "    feats2={'Aspect': 'Perf'}\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnccorpus[10018].tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size of -3 complete...\n",
      "window size of -2 complete...\n",
      "window size of -1 complete...\n",
      "window size of 0 complete...\n",
      "window size of 1 complete...\n",
      "window size of 2 complete...\n",
      "window size of 3 complete...\n"
     ]
    }
   ],
   "source": [
    "def build_sentence(tokens, upperlist):\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos == 'PUNCT':\n",
    "            space = ''\n",
    "        else:\n",
    "            space = ' '\n",
    "        if i in upperlist:\n",
    "            text = token.text.upper()\n",
    "        else:\n",
    "            text = token.text\n",
    "        if text == None:\n",
    "            text = ''\n",
    "        sentence = sentence + space + text\n",
    "    return sentence.strip()\n",
    "adpdict = {\n",
    "    'в' : 'в/во',\n",
    "    'во' : 'в/во',\n",
    "    'с' : 'с/со',\n",
    "    'со' : 'с/со',\n",
    "    'о' : 'о/об/',\n",
    "    'об' : 'о/об',\n",
    "    'обо' : 'о/об'\n",
    "}\n",
    "# sample = random.sample(rnccorpus,1000)\n",
    "# sample = rnccorpus[10018:10019]\n",
    "sample = rnccorpus\n",
    "windowList = []\n",
    "for window in range(-3, 4):\n",
    "    dataDict = dict()\n",
    "    for sent in sample:\n",
    "        currentindex = 0\n",
    "        for token in sent.tokens:\n",
    "            if token.pos == 'VERB':\n",
    "                verblemma = token.lemma\n",
    "                dataDict.setdefault(verblemma, dict())\n",
    "                dataDict[verblemma].setdefault('counts',dict())\n",
    "                dataDict[verblemma].setdefault('sentences',dict())\n",
    "                \n",
    "                adpindex = currentindex + window\n",
    "                if adpindex >= 0 and adpindex < len(sent.tokens):\n",
    "                    adptoken = sent.tokens[adpindex]\n",
    "                    if adptoken.pos == 'PUNCT' and adpindex + 1 >= 0 and adpindex + 1 < len(sent.tokens):\n",
    "                        adpindex = adpindex + 1\n",
    "                    if adptoken.pos == 'ADP':\n",
    "                        adplemma = adptoken.lemma\n",
    "                        # lemmatize prepositions\n",
    "                        if adplemma in adpdict:\n",
    "                            adplemma = adpdict[adplemma]\n",
    "                        caseindex = adpindex + 1\n",
    "                        if caseindex >= 0 and caseindex < len(sent.tokens):\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            if casetoken.pos == 'PUNCT' and caseindex + 1 >= 0 and caseindex + 1 < len(sent.tokens):\n",
    "                                caseindex = caseindex + 1\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            try:\n",
    "                                case = casetoken.feats['Case']\n",
    "                                if case == 'Ins':\n",
    "                                    case = 'Inst'\n",
    "                                adpandcase = adplemma + ' + ' + case.upper()\n",
    "                            except:\n",
    "                                break               \n",
    "                    else:\n",
    "                        if window in [-1,1]:\n",
    "                            # not an adposition directly before or after verb\n",
    "                            try: \n",
    "                                if adpindex - 1 >= 0:\n",
    "                                    # need to make sure prev token is not adp\n",
    "                                    if sent.tokens[adpindex - 1].pos != 'ADP':\n",
    "                                        case = adptoken.feats['Case']\n",
    "                                        if case == 'Ins':\n",
    "                                            case = 'Inst'\n",
    "                                        adpandcase = case.upper()\n",
    "                                        caseindex = currentindex\n",
    "                                    else:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    break\n",
    "                            except:\n",
    "                                # check if infinitive, if so, set caseindex to current index\n",
    "                                if adptoken.pos == 'VERB':\n",
    "                                    if 'VerbForm' in adptoken.feats and (adptoken.feats['VerbForm'] == 'Inf'):\n",
    "                                        adpandcase = 'INFINITIVE'\n",
    "                                        caseindex = currentindex\n",
    "                                else:\n",
    "                                    break\n",
    "                        else:\n",
    "                            break\n",
    "                    dataDict[verblemma]['counts'].setdefault(adpandcase, 0)\n",
    "                    dataDict[verblemma]['sentences'].setdefault(adpandcase, [])\n",
    "                    dataDict[verblemma]['counts'][adpandcase] += 1\n",
    "                    formattedsent = build_sentence(sent.tokens, [currentindex, adpindex, caseindex])\n",
    "                    if formattedsent not in dataDict[verblemma]['sentences'][adpandcase]:\n",
    "                        dataDict[verblemma]['sentences'][adpandcase].append(formattedsent)\n",
    "            currentindex += 1\n",
    "    print(f'window size of {window} complete...')\n",
    "    windowList.append(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7773\n"
     ]
    }
   ],
   "source": [
    "# removing uncommon verbs (<10 occurrences)\n",
    "uncommonVerbs = set()\n",
    "totalcount = dict()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        totalcount.setdefault(verb,0)\n",
    "        for cx in windowDict[verb]['counts']:\n",
    "            totalcount[verb] += windowDict[verb]['counts'][cx]\n",
    "for verb in totalcount:\n",
    "    if totalcount[verb] < 30:\n",
    "        uncommonVerbs.add(verb)\n",
    "print(len(uncommonVerbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verbtoremove in uncommonVerbs:\n",
    "    for windowDict in windowList:\n",
    "        try:\n",
    "            del windowDict[verbtoremove]\n",
    "        except:\n",
    "            False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windowList[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'про + ACC': 4,\n",
       " 'ACC': 69,\n",
       " 'NOM': 5,\n",
       " 'о/об/ + LOC': 5,\n",
       " 'DAT': 4,\n",
       " 'GEN': 4,\n",
       " 'у + GEN': 1,\n",
       " 'после + GEN': 1,\n",
       " 'с/со + GEN': 1,\n",
       " 'в/во + LOC': 4,\n",
       " 'без + GEN': 2,\n",
       " 'INST': 1,\n",
       " 'LOC': 1,\n",
       " 'о/об + LOC': 1}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowList[4]['читать']['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paigelee/Desktop/spring2021/clancy/verbhistograms'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcxlabels = set()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        for cxlabel in countdict:\n",
    "            allcxlabels.add(cxlabel)\n",
    "allcxlabels = list(allcxlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of cxx with count < 10 occurrences across verbs\n",
    "totalcxcounts = dict()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        for cx in countdict:\n",
    "            totalcxcounts.setdefault(cx, 0)\n",
    "            totalcxcounts[cx] += 1\n",
    "lst = []\n",
    "for cx in totalcxcounts:\n",
    "    lst.append((totalcxcounts[cx],cx))\n",
    "i = 0\n",
    "greaterthan10cx = []\n",
    "for ct, cx in lst:\n",
    "    if ct > 10:\n",
    "        greaterthan10cx.append(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb, windowsize, prep1, prep2, prep3, prep4, prep5...\n",
    "rows = []\n",
    "cxused = greaterthan10cx\n",
    "for windowIndex, windowDict in enumerate(windowList):\n",
    "    windowSize = windowIndex - 3\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        csvline = [verb, windowSize]\n",
    "        for cxlabel in cxused:\n",
    "            if cxlabel in countdict:\n",
    "                count = countdict[cxlabel]\n",
    "            else:\n",
    "                count = 0\n",
    "            csvline.append(count)\n",
    "        rows.append(csvline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кричать', 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "r = random.choice(rows)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Verb','WindowSize']\n",
    "\n",
    "for cxlabel in greaterthan10cx:\n",
    "    fields.append(cxlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paigelee/Desktop/spring2021/clancy/verbhistograms'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# writing to csv file \n",
    "filename = '7-29-21csvdata.csv'\n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(fields) \n",
    "        \n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/Desktop/spring2021/clancy/verbhistograms\n"
     ]
    }
   ],
   "source": [
    "cd verbhistograms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDict = dict()\n",
    "windowSize = -3\n",
    "for windowDict in windowList:\n",
    "    sentenceDict[windowSize] = dict()\n",
    "    for verb in windowDict:\n",
    "        sentenceDict[windowSize][verb] = dict()\n",
    "        for cx in windowDict[verb]['sentences']:\n",
    "            sentenceDict[windowSize][verb][cx] = windowDict[verb]['sentences'][cx][:10]\n",
    "    windowSize += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75653\n"
     ]
    }
   ],
   "source": [
    "totalsents = 0\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        for prep in windowDict[verb]['sentences']:\n",
    "            totalsents += len(windowDict[verb]['sentences'][prep])\n",
    "print(totalsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbDict = dict()\n",
    "\n",
    "for verb in windowList[0]:\n",
    "    verbDict[verb] = dict()\n",
    "windowSize = -3\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        verbDict[verb][windowSize] = dict()\n",
    "        for cx in windowDict[verb]['sentences']:\n",
    "            verbDict[verb][windowSize][cx] = list()\n",
    "            for sent in windowDict[verb]['sentences'][cx]:\n",
    "                verbDict[verb][windowSize][cx].append(sent)\n",
    "    windowSize += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for verb in verbDict:\n",
    "    with open(f'updatedsentdata2/{verb}.json', 'w', encoding='utf8') as jsonfile:\n",
    "        json.dump(verbDict[verb], jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write sentence dictionary to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences.json written successfully.\n"
     ]
    }
   ],
   "source": [
    "# # writing dictionary to json\n",
    "\n",
    "# import json\n",
    "\n",
    "# filepath = 'sentences.json'\n",
    "\n",
    "# with open(filepath, 'w', encoding='utf8') as json_file:\n",
    "#     json.dump(windowList, json_file, ensure_ascii=False)\n",
    "    \n",
    "# print(filepath,'written successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
