{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the RNC to look at preps in certain windows of verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98892"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from corus import load_morphoru_rnc\n",
    "\n",
    "path = 'RNCgoldInUD_Morpho.conll'\n",
    "records = load_morphoru_rnc(path)\n",
    "rnccorpus = []\n",
    "for record in records:\n",
    "    rnccorpus.append(record)\n",
    "len(rnccorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep + case!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_sentence(tokens, upperindexlist):\n",
    "#     sentence = ''\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         if token.pos == 'PUNCT':\n",
    "#             space = ''\n",
    "#         else:\n",
    "#             space = ' '\n",
    "#         if i in upperindexlist:\n",
    "#             text = token.text.upper()\n",
    "#         else:\n",
    "#             text = token.text\n",
    "#         if text == None:\n",
    "#             text = ''\n",
    "#         sentence = sentence + space + text\n",
    "#     return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8263828c6270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnccorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mformattedsent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'инструмент ПОСАДИТ ВАС на колени к черно-белым клавишам'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformattedsent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mthesetokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(rnccorpus):\n",
    "    formattedsent = build_sentence(sent.tokens,[])\n",
    "    if 'инструмент ПОСАДИТ ВАС на колени к черно-белым клавишам'.lower() in formattedsent.lower():\n",
    "        thesetokens = sent\n",
    "        index = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window size of -3 complete...\n",
      "window size of -2 complete...\n",
      "window size of -1 complete...\n",
      "window size of 0 complete...\n",
      "window size of 1 complete...\n",
      "window size of 2 complete...\n",
      "window size of 3 complete...\n"
     ]
    }
   ],
   "source": [
    "def build_sentence(tokens, upperlist=[]):\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos == 'PUNCT':\n",
    "            space = ''\n",
    "        else:\n",
    "            space = ' '\n",
    "        if i in upperlist:\n",
    "            text = token.text.upper()\n",
    "        else:\n",
    "            text = token.text\n",
    "        if text == None:\n",
    "            text = ''\n",
    "        sentence = sentence + space + text\n",
    "    return sentence.strip()\n",
    "adpdict = {\n",
    "    'в' : 'в/во',\n",
    "    'во' : 'в/во',\n",
    "    'с' : 'с/со',\n",
    "    'со' : 'с/со',\n",
    "    'о' : 'о/об',\n",
    "    'об' : 'о/об',\n",
    "    'обо' : 'о/об'\n",
    "}\n",
    "# sample = random.sample(rnccorpus,1000)\n",
    "# sample = rnccorpus[35557:35558]\n",
    "sample = rnccorpus\n",
    "windowList = []\n",
    "for window in range(-3, 4):\n",
    "    dataDict = dict()\n",
    "    for sent in sample:\n",
    "        currentindex = 0\n",
    "        for token in sent.tokens:\n",
    "            cxfound = False\n",
    "            if token.pos == 'VERB':\n",
    "                verblemma = token.lemma\n",
    "                dataDict.setdefault(verblemma, dict())\n",
    "                dataDict[verblemma].setdefault('counts',dict())\n",
    "                dataDict[verblemma].setdefault('sentences',dict())\n",
    "                \n",
    "                adpindex = currentindex + window\n",
    "                if adpindex >= 0 and adpindex < len(sent.tokens):\n",
    "                    adptoken = sent.tokens[adpindex]\n",
    "                    if adptoken.text != ',' and adptoken.pos == 'PUNCT' and adpindex + 1 >= 0 and adpindex + 1 < len(sent.tokens):\n",
    "                        adpindex = adpindex + 1\n",
    "                    if adptoken.pos == 'ADP':\n",
    "                        adplemma = adptoken.lemma\n",
    "                        # lemmatize prepositions\n",
    "                        if adplemma in adpdict:\n",
    "                            adplemma = adpdict[adplemma]\n",
    "                        caseindex = adpindex + 1\n",
    "                        if caseindex >= 0 and caseindex < len(sent.tokens):\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            if casetoken.pos == 'PUNCT' and caseindex + 1 >= 0 and caseindex + 1 < len(sent.tokens):\n",
    "                                caseindex = caseindex + 1\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            try:\n",
    "                                case = casetoken.feats['Case']\n",
    "                                if case == 'Ins':\n",
    "                                    case = 'Inst'\n",
    "                                adpandcase = adplemma + ' + ' + case.upper()\n",
    "                                cxfound = True    \n",
    "                            except:\n",
    "                                cxfound = False\n",
    "                    elif window == 1 and adptoken.text == ',':\n",
    "                        shtoindex = adpindex + 1\n",
    "                        if shtoindex >= 0 and shtoindex < len(sent.tokens):\n",
    "                            shtotoken = sent.tokens[shtoindex]\n",
    "                            if shtotoken.text == 'что':\n",
    "                                adpandcase = ', что'\n",
    "                                caseindex = shtoindex\n",
    "                                cxfound = True\n",
    "                    else:\n",
    "                        if window in [-1,1]:\n",
    "                            # not an adposition directly before or after verb\n",
    "                            try: \n",
    "                                if adpindex - 1 >= 0:\n",
    "                                    # need to make sure prev token is not adp\n",
    "                                    if sent.tokens[adpindex - 1].pos != 'ADP':\n",
    "                                        case = adptoken.feats['Case']\n",
    "                                        if case == 'Ins':\n",
    "                                            case = 'Inst'\n",
    "                                        adpandcase = case.upper()\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True    \n",
    "                            except:\n",
    "                                # check if infinitive, if so, set caseindex to current index\n",
    "                                if adptoken.pos == 'VERB':\n",
    "                                    if 'VerbForm' in adptoken.feats and (adptoken.feats['VerbForm'] == 'Inf'):\n",
    "                                        adpandcase = 'INFINITIVE'\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True\n",
    "                if cxfound:\n",
    "                    dataDict[verblemma]['counts'].setdefault(adpandcase, 0)\n",
    "                    dataDict[verblemma]['sentences'].setdefault(adpandcase, [])\n",
    "                    dataDict[verblemma]['counts'][adpandcase] += 1\n",
    "                    formattedsent = build_sentence(sent.tokens, [currentindex, adpindex, caseindex])\n",
    "                    if formattedsent not in dataDict[verblemma]['sentences'][adpandcase]:\n",
    "                        dataDict[verblemma]['sentences'][adpandcase].append(formattedsent)\n",
    "            currentindex += 1\n",
    "    print(f'window size of {window} complete...')\n",
    "    windowList.append(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Я ЗНАЮ, ЧТО с этим делать',\n",
       " 'а если нет- то хотя бы примерно ЗНАТЬ, ЧТО там',\n",
       " 'В общем, я не ЗНАЮ, ЧТО там, и не знаю, будешь ли ты их кому то- то передавать дальше- но не говори о том что это сделал я, особенно моей группе.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowList[4]['знать']['sentences'][', что'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5898\n"
     ]
    }
   ],
   "source": [
    "# removing uncommon verbs (<5 occurrences)\n",
    "uncommonVerbs = set()\n",
    "totalcount = dict()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        totalcount.setdefault(verb,0)\n",
    "        for cx in windowDict[verb]['counts']:\n",
    "            totalcount[verb] += windowDict[verb]['counts'][cx]\n",
    "for verb in totalcount:\n",
    "    if totalcount[verb] < 5:\n",
    "        uncommonVerbs.add(verb)\n",
    "print(len(uncommonVerbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verbtoremove in uncommonVerbs:\n",
    "    for windowDict in windowList:\n",
    "        try:\n",
    "            del windowDict[verbtoremove]\n",
    "        except:\n",
    "            False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3410"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windowList[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcxlabels = set()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        for cxlabel in countdict:\n",
    "            allcxlabels.add(cxlabel)\n",
    "allcxlabels = list(allcxlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of cxx with count < 10 occurrences across verbs\n",
    "totalcxcounts = dict()\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        for cx in countdict:\n",
    "            totalcxcounts.setdefault(cx, 0)\n",
    "            totalcxcounts[cx] += 1\n",
    "lst = []\n",
    "for cx in totalcxcounts:\n",
    "    lst.append((totalcxcounts[cx],cx))\n",
    "i = 0\n",
    "greaterthan10cx = []\n",
    "for ct, cx in lst:\n",
    "    if ct > 10:\n",
    "        greaterthan10cx.append(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb, windowsize, prep1, prep2, prep3, prep4, prep5...\n",
    "rows = []\n",
    "cxused = greaterthan10cx\n",
    "for windowIndex, windowDict in enumerate(windowList):\n",
    "    windowSize = windowIndex - 3\n",
    "    for verb in windowDict:\n",
    "        countdict = windowDict[verb]['counts']\n",
    "        csvline = [verb, windowSize]\n",
    "        for cxlabel in cxused:\n",
    "            if cxlabel in countdict:\n",
    "                count = countdict[cxlabel]\n",
    "            else:\n",
    "                count = 0\n",
    "            csvline.append(count)\n",
    "        rows.append(csvline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['усвоить', 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "r = random.choice(rows)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Verb','WindowSize']\n",
    "\n",
    "for cxlabel in greaterthan10cx:\n",
    "    fields.append(cxlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# writing to csv file \n",
    "filename = '8-16-21csvdata.csv'\n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(fields) \n",
    "        \n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paigelee/Desktop/spring2021/clancy/verbhistograms'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDict = dict()\n",
    "windowSize = -3\n",
    "for windowDict in windowList:\n",
    "    sentenceDict[windowSize] = dict()\n",
    "    for verb in windowDict:\n",
    "        sentenceDict[windowSize][verb] = dict()\n",
    "        for cx in windowDict[verb]['sentences']:\n",
    "            sentenceDict[windowSize][verb][cx] = windowDict[verb]['sentences'][cx][:10]\n",
    "    windowSize += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154787\n"
     ]
    }
   ],
   "source": [
    "totalsents = 0\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        for prep in windowDict[verb]['sentences']:\n",
    "            totalsents += len(windowDict[verb]['sentences'][prep])\n",
    "print(totalsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbDict = dict()\n",
    "\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        verbDict[verb] = dict()\n",
    "windowSize = -3\n",
    "for windowDict in windowList:\n",
    "    for verb in windowDict:\n",
    "        verbDict[verb][windowSize] = dict()\n",
    "        for cx in windowDict[verb]['sentences']:\n",
    "            verbDict[verb][windowSize][cx] = list()\n",
    "            for sent in windowDict[verb]['sentences'][cx]:\n",
    "                verbDict[verb][windowSize][cx].append(sent)\n",
    "    windowSize += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for verb in verbDict:\n",
    "    with open(f'sentdata8/{verb}.json', 'w', encoding='utf8') as jsonfile:\n",
    "        json.dump(verbDict[verb], jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get most common constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98892 sentences parsed...\n",
      "10000/98892 sentences parsed...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6c089ea8e08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossiblecx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mcommonDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverblemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mcommonDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverblemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mcommonDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverblemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def build_sentence(tokens, upperlist):\n",
    "#     sentence = ''\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         if token.pos == 'PUNCT':\n",
    "#             space = ''\n",
    "#         else:\n",
    "#             space = ' '\n",
    "#         if i in upperlist:\n",
    "#             text = token.text.upper()\n",
    "#         else:\n",
    "#             text = token.text\n",
    "#         if text == None:\n",
    "#             text = ''\n",
    "#         sentence = sentence + space + text\n",
    "#     return sentence.strip()\n",
    "# adpdict = {\n",
    "#     'в' : 'в/во',\n",
    "#     'во' : 'в/во',\n",
    "#     'с' : 'с/со',\n",
    "#     'со' : 'с/со',\n",
    "#     'о' : 'о/об',\n",
    "#     'об' : 'о/об',\n",
    "#     'обо' : 'о/об'\n",
    "# }\n",
    "# # sample = random.sample(rnccorpus,1000)\n",
    "# # sample = rnccorpus[10018:10025]\n",
    "# sample = rnccorpus\n",
    "\n",
    "# commonDict = dict()\n",
    "# for i, sent in enumerate(sample):\n",
    "#     if i % 10000 == 0:\n",
    "#         print(f'{i}/{len(sample)} sentences parsed...')\n",
    "#     currentindex = 0\n",
    "#     for token in sent.tokens:\n",
    "#         # for each verb found\n",
    "#         if token.pos == 'VERB':\n",
    "#             verblemma = token.lemma\n",
    "#             commonDict.setdefault(verblemma, dict())\n",
    "#             before = []\n",
    "#             after = []\n",
    "#             for window in range(-3,4):\n",
    "#                 cxfound = False\n",
    "#                 adpindex = currentindex + window\n",
    "\n",
    "#                 if adpindex >= 0 and adpindex < len(sent.tokens):\n",
    "#                     adptoken = sent.tokens[adpindex]\n",
    "#                     if adptoken.text != ',' and adptoken.pos == 'PUNCT' and adpindex + 1 >= 0 and adpindex + 1 < len(sent.tokens):\n",
    "#                         adpindex = adpindex + 1\n",
    "#                     if adptoken.pos == 'ADP':\n",
    "#                         adplemma = adptoken.lemma\n",
    "#                         # lemmatize prepositions\n",
    "#                         if adplemma in adpdict:\n",
    "#                             adplemma = adpdict[adplemma]\n",
    "#                         caseindex = adpindex + 1\n",
    "#                         if caseindex >= 0 and caseindex < len(sent.tokens):\n",
    "#                             casetoken = sent.tokens[caseindex]\n",
    "#                             if casetoken.pos == 'PUNCT' and caseindex + 1 >= 0 and caseindex + 1 < len(sent.tokens):\n",
    "#                                 caseindex = caseindex + 1\n",
    "#                             casetoken = sent.tokens[caseindex]\n",
    "#                             try:\n",
    "#                                 case = casetoken.feats['Case']\n",
    "#                                 if case == 'Ins':\n",
    "#                                     case = 'Inst'\n",
    "#                                 adpandcase = adplemma + ' + ' + case.upper()\n",
    "#                                 cxfound = True    \n",
    "#                             except:\n",
    "#                                 cxfound = False\n",
    "#                     elif adptoken.text == ',':\n",
    "#                         shtoindex = adpindex + 1\n",
    "#                         if shtoindex >= 0 and shtoindex < len(sent.tokens):\n",
    "#                             shtotoken = sent.tokens[shtoindex]\n",
    "#                             if shtotoken.text == 'что':\n",
    "#                                 adpandcase = ', что'\n",
    "#                                 cxfound = True\n",
    "#                     else:\n",
    "#                         if window in [-1,1]:\n",
    "#                             # not an adposition directly before or after verb\n",
    "#                             try: \n",
    "#                                 if adpindex >= 0:\n",
    "#                                     # need to make sure prev token is not adp\n",
    "#                                     if sent.tokens[adpindex - 1].pos != 'ADP':\n",
    "#                                         case = adptoken.feats['Case']\n",
    "#                                         if case == 'Ins':\n",
    "#                                             case = 'Inst'\n",
    "#                                         adpandcase = case.upper()\n",
    "#                                         caseindex = currentindex\n",
    "#                                         cxfound = True\n",
    "#                             except:\n",
    "#                                 # check if infinitive, if so, set caseindex to current index\n",
    "#                                 if adptoken.pos == 'VERB':\n",
    "#                                     if 'VerbForm' in adptoken.feats and (adptoken.feats['VerbForm'] == 'Inf'):\n",
    "#                                         adpandcase = 'INFINITIVE'\n",
    "#                                         caseindex = currentindex\n",
    "#                                         cxfound = True\n",
    "#                 if cxfound:\n",
    "#                     if window < 0:\n",
    "#                         before.append(adpandcase)\n",
    "#                     else:\n",
    "#                         after.append(adpandcase)\n",
    "# #                 dataDict[verblemma]['counts'].setdefault(adpandcase, 0)\n",
    "# #                 dataDict[verblemma]['sentences'].setdefault(adpandcase, [])\n",
    "# #                 dataDict[verblemma]['counts'][adpandcase] += 1\n",
    "# #                 formattedsent = build_sentence(sent.tokens, [currentindex, adpindex, caseindex])\n",
    "# #                 if formattedsent not in dataDict[verblemma]['sentences'][adpandcase]:\n",
    "# #                     dataDict[verblemma]['sentences'][adpandcase].append(formattedsent)\n",
    "#             possiblecx = set()\n",
    "#             if before == []:\n",
    "#                 for af in after:\n",
    "#                     possiblecx.add(f'{verblemma} + {af}')\n",
    "#             elif after == []:\n",
    "#                 for be in before:\n",
    "#                     possiblecx.add(f'{be} + {verblemma}')\n",
    "#             else:\n",
    "#                 for be in before: \n",
    "#                     for af in after:\n",
    "#                         possiblecx.add(f'{be} + {verblemma} + {af}')\n",
    "                                   \n",
    "#             for cx in possiblecx:\n",
    "#                 commonDict[verblemma].setdefault(cx, dict())\n",
    "#                 commonDict[verblemma][cx].setdefault('counts',0)\n",
    "#                 commonDict[verblemma][cx].setdefault('sentences',set())\n",
    "#                 commonDict[verblemma][cx]['counts'] += 1\n",
    "#                 commonDict[verblemma][cx]['sentences'].add(build_sentence(sent.tokens, [currentindex]))\n",
    "#         currentindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['посадить + в/во + ACC', 'посадить + под + ACC', 'посадить + ACC', 'посадить + у + GEN', 'INST + посадить + на + ACC', 'INST + посадить + ACC', 'NOM + посадить + на + ACC', 'посадить + с/со + INST', 'ACC + посадить', 'NOM + посадить + ACC', 'посадить + на + ACC', 'ACC + посадить + в/во + ACC', 'ACC + посадить + ACC', 'на + ACC + посадить + ACC', 'между + INST + посадить + ACC', 'в/во + LOC + посадить + ACC', 'NOM + посадить', 'посадить + INST', 'между + INST + посадить', 'ACC + посадить + на + LOC', 'в/во + ACC + посадить', 'с/со + INST + посадить', 'посадить + для + GEN', 'ACC + посадить + с/со + INST'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonDict['посадить'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDict = dict()\n",
    "for verb in commonDict:\n",
    "    cxlst = []\n",
    "    for cx in commonDict[verb]:\n",
    "        cxlst.append((commonDict[verb][cx]['counts'], cx)) \n",
    "        commonDict[verb][cx]['sentences'] = list(commonDict[verb][cx]['sentences'])\n",
    "    sortedlist = sorted(cxlst, reverse=True)\n",
    "    top3 = sortedlist[:3]\n",
    "    jsonDict[verb] = []\n",
    "    for tup in top3:\n",
    "        jsonDict[verb].append({\n",
    "            'cx' : tup[1],\n",
    "            'data' : commonDict[verb][tup[1]]\n",
    "        })\n",
    "jsonList = [jsonDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paigelee/Desktop/spring2021/clancy/verbhistograms'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'common_constructions.json', 'w', encoding='utf8') as jsonfile:\n",
    "    json.dump(jsonList, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### constructions 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98892 sentences parsed...\n",
      "10000/98892 sentences parsed...\n",
      "20000/98892 sentences parsed...\n",
      "30000/98892 sentences parsed...\n",
      "40000/98892 sentences parsed...\n",
      "50000/98892 sentences parsed...\n",
      "60000/98892 sentences parsed...\n",
      "70000/98892 sentences parsed...\n",
      "80000/98892 sentences parsed...\n",
      "90000/98892 sentences parsed...\n"
     ]
    }
   ],
   "source": [
    "def build_sentence(tokens, upperlist):\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos == 'PUNCT':\n",
    "            space = ''\n",
    "        else:\n",
    "            space = ' '\n",
    "        if i in upperlist:\n",
    "            text = token.text.upper()\n",
    "        else:\n",
    "            text = token.text\n",
    "        if text == None:\n",
    "            text = ''\n",
    "        sentence = sentence + space + text\n",
    "    return sentence.strip()\n",
    "adpdict = {\n",
    "    'в' : 'в/во',\n",
    "    'во' : 'в/во',\n",
    "    'с' : 'с/со',\n",
    "    'со' : 'с/со',\n",
    "    'о' : 'о/об',\n",
    "    'об' : 'о/об',\n",
    "    'обо' : 'о/об'\n",
    "}\n",
    "# sample = random.sample(rnccorpus,1000)\n",
    "# sample = rnccorpus[35557:35558]\n",
    "sample = rnccorpus\n",
    "\n",
    "commonDict = dict()\n",
    "for i, sent in enumerate(sample):\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i}/{len(sample)} sentences parsed...')\n",
    "    currentindex = 0\n",
    "    for token in sent.tokens:\n",
    "        # for each verb found\n",
    "        if token.pos == 'VERB':\n",
    "            verblemma = token.lemma\n",
    "            commonDict.setdefault(verblemma, dict())\n",
    "            allcxx = []\n",
    "            orderedcx = []\n",
    "            for window in range(-3,4):\n",
    "                cxfound = False\n",
    "                adpindex = currentindex + window\n",
    "\n",
    "                if adpindex >= 0 and adpindex < len(sent.tokens):\n",
    "                    adptoken = sent.tokens[adpindex]\n",
    "                    if adptoken.text != ',' and adptoken.pos == 'PUNCT' and adpindex + 1 >= 0 and adpindex + 1 < len(sent.tokens):\n",
    "                        adpindex = adpindex + 1\n",
    "                    if adptoken.pos == 'ADP':\n",
    "                        adplemma = adptoken.lemma\n",
    "                        # lemmatize prepositions\n",
    "                        if adplemma in adpdict:\n",
    "                            adplemma = adpdict[adplemma]\n",
    "                        caseindex = adpindex + 1\n",
    "                        if caseindex >= 0 and caseindex < len(sent.tokens):\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            if casetoken.pos == 'PUNCT' and caseindex + 1 >= 0 and caseindex + 1 < len(sent.tokens):\n",
    "                                caseindex = caseindex + 1\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            try:\n",
    "                                case = casetoken.feats['Case']\n",
    "                                if case == 'Ins':\n",
    "                                    case = 'Inst'\n",
    "                                adpandcase = adplemma + ' + ' + case.upper()\n",
    "                                cxfound = True    \n",
    "                            except:\n",
    "                                cxfound = False\n",
    "                    elif window == 1 and adptoken.text == ',':\n",
    "                        shtoindex = adpindex + 1\n",
    "                        if shtoindex >= 0 and shtoindex < len(sent.tokens):\n",
    "                            shtotoken = sent.tokens[shtoindex]\n",
    "                            if shtotoken.text == 'что':\n",
    "                                adpandcase = ', что'\n",
    "                                cxfound = True\n",
    "                    else:\n",
    "                        if window in [-1,1]:\n",
    "                            # not an adposition directly before or after verb\n",
    "                            try: \n",
    "                                if adpindex >= 0:\n",
    "                                    # need to make sure prev token is not adp\n",
    "                                    if sent.tokens[adpindex - 1].pos != 'ADP':\n",
    "                                        case = adptoken.feats['Case']\n",
    "                                        if case == 'Ins':\n",
    "                                            case = 'Inst'\n",
    "                                        adpandcase = case.upper()\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True\n",
    "                            except:\n",
    "                                # check if infinitive, if so, set caseindex to current index\n",
    "                                if adptoken.pos == 'VERB':\n",
    "                                    if 'VerbForm' in adptoken.feats and (adptoken.feats['VerbForm'] == 'Inf'):\n",
    "                                        adpandcase = 'INFINITIVE'\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True\n",
    "                if cxfound:\n",
    "                    allcxx.append((adpandcase,adpindex,caseindex))\n",
    "                    \n",
    "            for (cx, adpindex, caseindex) in allcxx:\n",
    "                commonDict[verblemma].setdefault(cx, dict())\n",
    "                commonDict[verblemma][cx].setdefault('counts',0)\n",
    "                commonDict[verblemma][cx].setdefault('sentences',set())\n",
    "                commonDict[verblemma][cx]['counts'] += 1\n",
    "                commonDict[verblemma][cx]['sentences'].add(build_sentence(sent.tokens, [currentindex, adpindex, caseindex]))\n",
    "                \n",
    "                \n",
    "        currentindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['о/об + LOC', 'NOM', 'в/во + ACC', 'INFINITIVE', 'в/во + LOC', 'при + LOC'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonDict['заботиться'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get highest numbered things / critical number\n",
    "jsonDict = dict()\n",
    "for verb in commonDict:\n",
    "    cxlist = []\n",
    "    for cx in commonDict[verb]:\n",
    "        cxlist.append((commonDict[verb][cx]['counts'], cx))\n",
    "    sortedlst = sorted(cxlist, reverse=True)\n",
    "    newcx = verb\n",
    "    for ct, cx in sortedlst[:3]:\n",
    "        if ct < 3:\n",
    "            continue\n",
    "        if cx == 'NOM':\n",
    "            newcx = cx + ' + ' + newcx\n",
    "        else:\n",
    "            newcx = newcx + ' + ' + cx\n",
    "    if 'NOM' not in newcx:\n",
    "        if 'NOM' in [cx for ct, cx in sortedlst[:5]]:\n",
    "            newcx = 'NOM + ' + newcx\n",
    "    jsonDict[verb] = newcx\n",
    "jsonList = [jsonDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пожертвовать + INST'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonDict['пожертвовать']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'common_constructions1.json', 'w', encoding='utf8') as jsonfile:\n",
    "    json.dump(jsonList, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ordered CX examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98892 sentences parsed...\n",
      "10000/98892 sentences parsed...\n",
      "20000/98892 sentences parsed...\n",
      "30000/98892 sentences parsed...\n",
      "40000/98892 sentences parsed...\n",
      "50000/98892 sentences parsed...\n",
      "60000/98892 sentences parsed...\n",
      "70000/98892 sentences parsed...\n",
      "80000/98892 sentences parsed...\n",
      "90000/98892 sentences parsed...\n"
     ]
    }
   ],
   "source": [
    "def build_sentence(tokens, upperlist):\n",
    "    sentence = ''\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos == 'PUNCT':\n",
    "            space = ''\n",
    "        else:\n",
    "            space = ' '\n",
    "        if i in upperlist:\n",
    "            text = token.text.upper()\n",
    "        else:\n",
    "            text = token.text\n",
    "        if text == None:\n",
    "            text = ''\n",
    "        sentence = sentence + space + text\n",
    "    return sentence.strip()\n",
    "adpdict = {\n",
    "    'в' : 'в/во',\n",
    "    'во' : 'в/во',\n",
    "    'с' : 'с/со',\n",
    "    'со' : 'с/со',\n",
    "    'о' : 'о/об',\n",
    "    'об' : 'о/об',\n",
    "    'обо' : 'о/об'\n",
    "}\n",
    "# sample = random.sample(rnccorpus,1000)\n",
    "# sample = rnccorpus[35557:35558]\n",
    "sample = rnccorpus\n",
    "\n",
    "orderedDict = dict()\n",
    "for i, sent in enumerate(sample):\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i}/{len(sample)} sentences parsed...')\n",
    "    currentindex = 0\n",
    "    for token in sent.tokens:\n",
    "        # for each verb found\n",
    "        if token.pos == 'VERB':\n",
    "            verblemma = token.lemma\n",
    "            orderedDict.setdefault(verblemma, dict())\n",
    "            before = []\n",
    "            after = []\n",
    "            for window in range(-3,4):\n",
    "                cxfound = False\n",
    "                adpindex = currentindex + window\n",
    "\n",
    "                if adpindex >= 0 and adpindex < len(sent.tokens):\n",
    "                    adptoken = sent.tokens[adpindex]\n",
    "                    if adptoken.text != ',' and adptoken.pos == 'PUNCT' and adpindex + 1 >= 0 and adpindex + 1 < len(sent.tokens):\n",
    "                        adpindex = adpindex + 1\n",
    "                    if adptoken.pos == 'ADP':\n",
    "                        adplemma = adptoken.lemma\n",
    "                        # lemmatize prepositions\n",
    "                        if adplemma in adpdict:\n",
    "                            adplemma = adpdict[adplemma]\n",
    "                        caseindex = adpindex + 1\n",
    "                        if caseindex >= 0 and caseindex < len(sent.tokens):\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            if casetoken.pos == 'PUNCT' and caseindex + 1 >= 0 and caseindex + 1 < len(sent.tokens):\n",
    "                                caseindex = caseindex + 1\n",
    "                            casetoken = sent.tokens[caseindex]\n",
    "                            try:\n",
    "                                case = casetoken.feats['Case']\n",
    "                                if case == 'Ins':\n",
    "                                    case = 'Inst'\n",
    "                                adpandcase = adplemma + ' + ' + case.upper()\n",
    "                                cxfound = True    \n",
    "                            except:\n",
    "                                cxfound = False\n",
    "                    elif window == 1 and adptoken.text == ',':\n",
    "                        shtoindex = adpindex + 1\n",
    "                        if shtoindex >= 0 and shtoindex < len(sent.tokens):\n",
    "                            shtotoken = sent.tokens[shtoindex]\n",
    "                            if shtotoken.text == 'что':\n",
    "                                adpandcase = ', что'\n",
    "                                caseindex = shtoindex\n",
    "                                cxfound = True\n",
    "                    else:\n",
    "                        if window in [-1,1]:\n",
    "                            # not an adposition directly before or after verb\n",
    "                            try: \n",
    "                                if adpindex >= 0:\n",
    "                                    # need to make sure prev token is not adp\n",
    "                                    if sent.tokens[adpindex - 1].pos != 'ADP':\n",
    "                                        case = adptoken.feats['Case']\n",
    "                                        if case == 'Ins':\n",
    "                                            case = 'Inst'\n",
    "                                        adpandcase = case.upper()\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True\n",
    "                            except:\n",
    "                                # check if infinitive, if so, set caseindex to current index\n",
    "                                if adptoken.pos == 'VERB':\n",
    "                                    if 'VerbForm' in adptoken.feats and (adptoken.feats['VerbForm'] == 'Inf'):\n",
    "                                        adpandcase = 'INFINITIVE'\n",
    "                                        caseindex = currentindex\n",
    "                                        cxfound = True\n",
    "                if cxfound:\n",
    "                    if window < 0:\n",
    "                        before.append((adpandcase, adpindex, caseindex))\n",
    "                    else:\n",
    "                        after.append((adpandcase, adpindex, caseindex))\n",
    "            orderedcx = ''\n",
    "            allindices = [currentindex]\n",
    "            if before != []:\n",
    "                for cx, adpindex, caseindex in before:\n",
    "                    orderedcx = orderedcx + cx + ' + '\n",
    "                    allindices.append(adpindex)\n",
    "                    allindices.append(caseindex)\n",
    "            orderedcx = orderedcx + verblemma\n",
    "            if after != []:\n",
    "                for cx, adpindex, caseindex in after:\n",
    "                    orderedcx = orderedcx + ' + ' + cx\n",
    "                    allindices.append(adpindex)\n",
    "                    allindices.append(caseindex)\n",
    "\n",
    "            orderedDict[verblemma].setdefault(orderedcx, dict())\n",
    "            orderedDict[verblemma][orderedcx].setdefault('counts',0)\n",
    "            orderedDict[verblemma][orderedcx].setdefault('sentences',set())\n",
    "            orderedDict[verblemma][orderedcx]['counts'] += 1\n",
    "            orderedDict[verblemma][orderedcx]['sentences'].add(build_sentence(sent.tokens, allindices))\n",
    "                \n",
    "                \n",
    "        currentindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['расступиться',\n",
       " 'самоопределиться',\n",
       " 'караться',\n",
       " 'втягиваться',\n",
       " 'подкорректировать',\n",
       " 'расчленять',\n",
       " 'щупать',\n",
       " 'одерживать',\n",
       " 'набиваться',\n",
       " 'поднадоесть']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(uncommonVerbs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxlst = []\n",
    "topDict = dict()\n",
    "for verb in orderedDict:\n",
    "    if verb in uncommonVerbs:\n",
    "        continue\n",
    "    for cx in orderedDict[verb]:\n",
    "        cxlst.append((orderedDict[verb][cx]['counts'], cx))\n",
    "    sortedcxlst = sorted(cxlst, reverse=True)\n",
    "    topDict[verb] = sortedcxlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = 'думать'\n",
    "cxlst = []\n",
    "for cx in orderedDict[verb]:\n",
    "    cxlst.append((orderedDict[verb][cx]['counts'], cx))\n",
    "newlist = sorted(cxlst, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276, 'NOM + думать'),\n",
       " (273, 'думать'),\n",
       " (61, 'думать + о/об + LOC'),\n",
       " (59, 'NOM + думать + , что'),\n",
       " (58, 'думать + , что'),\n",
       " (34, 'думать + NOM'),\n",
       " (30, 'NOM + думать + о/об + LOC'),\n",
       " (28, 'о/об + LOC + думать'),\n",
       " (13, 'NOM + думать + в/во + LOC'),\n",
       " (12, 'ACC + думать'),\n",
       " (10, 'NOM + думать + у + GEN'),\n",
       " (9, 'NOM + думать + по + DAT'),\n",
       " (7, 'NOM + думать + NOM'),\n",
       " (6, 'думать + у + GEN'),\n",
       " (5, 'думать + про + ACC'),\n",
       " (5, 'ACC + думать + по + DAT'),\n",
       " (4, 'думать + в/во + ACC'),\n",
       " (4, 'думать + INFINITIVE'),\n",
       " (4, 'думать + , что + в/во + LOC'),\n",
       " (4, 'в/во + LOC + думать'),\n",
       " (4, 'NOM + думать + на + ACC'),\n",
       " (4, 'NOM + думать + для + GEN'),\n",
       " (3, 'на + ACC + думать'),\n",
       " (3, 'думать + от + GEN'),\n",
       " (3, 'думать + в/во + LOC'),\n",
       " (3, 'думать + ACC'),\n",
       " (3, 'NOM + думать + с/со + INST'),\n",
       " (3, 'NOM + думать + про + ACC'),\n",
       " (3, 'ACC + думать + о/об + LOC'),\n",
       " (2, 'с/со + INST + думать + о/об + LOC'),\n",
       " (2, 'с/со + GEN + думать'),\n",
       " (2, 'по + DAT + DAT + думать'),\n",
       " (2, 'о/об + LOC + думать + NOM'),\n",
       " (2, 'над + INST + думать'),\n",
       " (2, 'на + LOC + думать'),\n",
       " (2, 'думать + с/со + INST'),\n",
       " (2, 'думать + из + GEN'),\n",
       " (2, 'думать + за + ACC'),\n",
       " (2, 'думать + NOM + с/со + INST'),\n",
       " (2, 'NOM + думать + к + DAT'),\n",
       " (2, 'NOM + думать + в/во + ACC'),\n",
       " (2, 'NOM + думать + без + GEN'),\n",
       " (2, 'NOM + думать + , что + у + GEN'),\n",
       " (2, 'NOM + думать + , что + на + LOC'),\n",
       " (2, 'NOM + думать + , что + в/во + LOC'),\n",
       " (2, 'ACC + думать + NOM'),\n",
       " (1, 'у + GEN + думать + NOM'),\n",
       " (1, 'у + GEN + думать'),\n",
       " (1, 'с/со + INST + думать + про + ACC'),\n",
       " (1, 'с/со + INST + думать + NOM'),\n",
       " (1, 'с/со + INST + думать + , что'),\n",
       " (1, 'с/со + INST + думать'),\n",
       " (1, 'с/со + INST + INST + думать + , что'),\n",
       " (1, 'про + ACC + думать + , что'),\n",
       " (1, 'про + ACC + думать'),\n",
       " (1, 'по + DAT + думать + ACC'),\n",
       " (1, 'по + DAT + GEN + думать'),\n",
       " (1, 'о/об + LOC + думать + про + ACC'),\n",
       " (1, 'о/об + LOC + думать + перед + INST'),\n",
       " (1, 'о/об + LOC + думать + о/об + LOC'),\n",
       " (1, 'о/об + LOC + думать + ACC'),\n",
       " (1, 'о/об + LOC + думать + , что'),\n",
       " (1, 'о/об + LOC + NOM + думать'),\n",
       " (1, 'о/об + ACC + думать'),\n",
       " (1, 'на + LOC + думать + , что + от + GEN'),\n",
       " (1, 'на + LOC + GEN + думать + над + INST'),\n",
       " (1, 'на + ACC + думать + в/во + LOC'),\n",
       " (1, 'из + GEN + думать + ACC'),\n",
       " (1, 'из + GEN + думать'),\n",
       " (1, 'из + GEN + GEN + думать'),\n",
       " (1, 'думать + при + LOC'),\n",
       " (1, 'думать + перед + INST'),\n",
       " (1, 'думать + около + GEN'),\n",
       " (1, 'думать + о/об + LOC + при + LOC'),\n",
       " (1, 'думать + насчет + GEN'),\n",
       " (1, 'думать + над + INST'),\n",
       " (1, 'думать + на + LOC'),\n",
       " (1, 'думать + на + ACC'),\n",
       " (1, 'думать + до + GEN'),\n",
       " (1, 'думать + без + GEN + в/во + LOC'),\n",
       " (1, 'думать + без + GEN'),\n",
       " (1, 'думать + NOM + у + GEN'),\n",
       " (1, 'думать + NOM + о/об + LOC'),\n",
       " (1, 'думать + NOM + на + LOC'),\n",
       " (1, 'думать + INST + о/об + LOC'),\n",
       " (1, 'думать + INFINITIVE + с/со + INST'),\n",
       " (1, 'думать + INFINITIVE + из + GEN'),\n",
       " (1, 'думать + DAT'),\n",
       " (1, 'думать + , что + у + GEN'),\n",
       " (1, 'думать + , что + с/со + INST'),\n",
       " (1, 'думать + , что + на + ACC'),\n",
       " (1, 'в/во + LOC + LOC + думать'),\n",
       " (1, 'в/во + ACC + думать + , что'),\n",
       " (1, 'в/во + ACC + думать'),\n",
       " (1, 'без + GEN + думать + , что'),\n",
       " (1, 'NOM + думать + через + ACC'),\n",
       " (1, 'NOM + думать + среди + GEN'),\n",
       " (1, 'NOM + думать + с/со + INST + в/во + LOC'),\n",
       " (1, 'NOM + думать + о/об + LOC + с/со + GEN'),\n",
       " (1, 'NOM + думать + над + INST'),\n",
       " (1, 'NOM + думать + из + GEN'),\n",
       " (1, 'NOM + думать + за + ACC'),\n",
       " (1, 'NOM + думать + в/во + NOM'),\n",
       " (1, 'NOM + думать + INFINITIVE'),\n",
       " (1, 'NOM + думать + GEN'),\n",
       " (1, 'NOM + думать + , что + с/со + GEN'),\n",
       " (1, 'NOM + думать + , что + при + LOC'),\n",
       " (1, 'NOM + думать + , что + для + GEN'),\n",
       " (1, 'NOM + думать + , что + в/во + ACC'),\n",
       " (1, 'INST + думать'),\n",
       " (1, 'GEN + думать + на + LOC'),\n",
       " (1, 'ACC + думать + ACC'),\n",
       " (1, 'ACC + думать + , что')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'быть'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-89ac7820cd4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morderedDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'думать'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'быть'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'быть'"
     ]
    }
   ],
   "source": [
    "orderedDict['думать']['быть']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
